
Name: Sagar Giradkar
RollNo: 3051
Ass: Perform Map Reduce on word count using Java


////////////////////////Driver.java/////////////////////////////

import java.io.IOException;
import java.net.URI;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;


public class myDriver {

	@SuppressWarnings("deprecation")
	public static void main(String[] args) throws Exception{
		Configuration c = new Configuration();
		Job job = new Job(c, "WordCount");
		job.setJarByClass(myDriver.class);
		job.setMapperClass(myMapper.class);
		job.setReducerClass(myReducer.class);
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
//		System.exit(job.waitForCompletion(true)?0:1);
		job.waitForCompletion(true);
		
		FileSystem fs= FileSystem.get(URI.create("hdfs://localhost:9000"+args[1]),c);
	    String Fname=fs.listStatus(new Path(args[1]))[1].getPath().toString();
	    							
	    FSDataInputStream in=null;
	    							
    	try
	    {
	    in=fs.open(new Path(Fname));
	    
	    String maxIp=null;
	    int maxCount=-1,intCount=-1;
	    int i=0;
	    
	    String line=null;
	    
	    while ((line=in.readLine())!=null)
	    {
	    	String words[]=line.split("\t");
	    	String ip =words[0];
	    	String cnt=words[1];
	    	
	    	intCount=Integer.parseInt(cnt);
	    	if(i == 0)
	    	{
	    		maxCount=intCount;
	    		maxIp=ip;
	    		i++;
	    	}
	    	else {
	    		if(intCount > maxCount) {
	    			maxCount = intCount;
	    			maxIp = ip;
	    		}
	    	}
	    }
	    
	    System.out.println("Max No. of logged in ip="+maxIp+" with count: "+maxCount);
	    in.close();
	    }
    	catch(IOException except)
    	{
    		except.printStackTrace();
    	}
	}

}

//////////////////////////Mapper.java///////////////////////////////////////

import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class myMapper extends Mapper<LongWritable, Text, Text, IntWritable>{
	
	public void map(LongWritable offset, Text key, Context con) throws IOException, InterruptedException{
		String line = key.toString();
		String [] words = line.split(" ");
		Text outputKey = new Text(words[0]);
		IntWritable outputValue = new IntWritable(1);
		con.write(outputKey,  outputValue);
//		for(String word: words) {
//			Text outputKey = new Text(word);
//			IntWritable outputValue = new IntWritable(1);
//			con.write(outputKey,  outputValue);
//		}
	}

}

	
///////////////////////////Reducer.java///////////////////////////////////

import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class myReducer extends Reducer<Text, IntWritable, Text, IntWritable>{

	public void reduce(Text word, Iterable<IntWritable> values, Context con) throws IOException, InterruptedException{
		int sum = 0;
		for (IntWritable value: values) {
			sum += value.get();
		}
		con.write(word,  new IntWritable(sum));
	}
}
	
	
//////////////////////////////OUPUT OF Logfile////////////////////////////////////////////

hduser@student-ThinkCentre-M700:~$ hadoop jar /home/hduser/Downloads/logfile.jar 
MaxLoginTime /mydata/log.txt /mydata/output1
2024-04-20 16:30:52,162 WARN util.NativeCodeLoader: Unable to load native-hadoop library 
for your platform... using builtin-java classes where applicable
2024-04-20 16:30:52,752 INFO impl.MetricsConfig: Loaded properties from hadoopmetrics2.properties
2024-04-20 16:30:52,817 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 
10 second(s).
2024-04-20 16:30:52,818 INFO impl.MetricsSystemImpl: JobTracker metrics system started
2024-04-20 16:30:52,950 WARN mapreduce.JobResourceUploader: Hadoop command-line 
option parsing not performed. Implement the Tool interface and execute your application with 
ToolRunner to remedy this.
2024-04-20 16:30:53,046 INFO input.FileInputFormat: Total input files to process : 1
2024-04-20 16:30:53,203 INFO mapreduce.JobSubmitter: number of splits:1
2024-04-20 16:30:53,411 INFO mapreduce.JobSubmitter: Submitting tokens for job: 
job_local1472617565_0001
2024-04-20 16:30:53,411 INFO mapreduce.JobSubmitter: Executing with tokens: []
2024-04-20 16:30:53,592 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
2024-04-20 16:30:53,593 INFO mapreduce.Job: Running job: job_local1472617565_0001
2024-04-20 16:30:53,594 INFO mapred.LocalJobRunner: OutputCommitter set in config null
2024-04-20 16:30:53,634 INFO output.FileOutputCommitter: File Output Committer Algorithm 
version is 2
2024-04-20 16:30:53,634 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup 
_temporary folders under output directory:false, ignore cleanup failures: false
2024-04-20 16:30:53,635 INFO mapred.LocalJobRunner: OutputCommitter is 
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2024-04-20 16:30:53,695 INFO mapred.LocalJobRunner: Waiting for map tasks
2024-04-20 16:30:53,696 INFO mapred.LocalJobRunner: Starting task: 
attempt_local1472617565_0001_m_000000_0
2024-04-20 16:30:53,718 INFO output.FileOutputCommitter: File Output Committer Algorithm 
version is 2
2024-04-20 16:30:53,718 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup 
_temporary folders under output directory:false, ignore cleanup failures: false
2024-04-20 16:30:53,770 INFO mapred.Task: Using ResourceCalculatorProcessTree : [ ]
2024-04-20 16:30:53,776 INFO mapred.MapTask: Processing split: 
hdfs://localhost:9000/mydata/log.txt:0+143084
2024-04-20 16:30:53,821 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2024-04-20 16:30:53,821 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2024-04-20 16:30:53,821 INFO mapred.MapTask: soft limit at 83886080
2024-04-20 16:30:53,821 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2024-04-20 16:30:53,821 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2024-04-20 16:30:53,825 INFO mapred.MapTask: Map output collector class = 
org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2024-04-20 16:30:54,340 INFO mapred.LocalJobRunner:
2024-04-20 16:30:54,345 INFO mapred.MapTask: Starting flush of map output
2024-04-20 16:30:54,345 INFO mapred.MapTask: Spilling map output
2024-04-20 16:30:54,345 INFO mapred.MapTask: bufstart = 0; bufend = 22493; bufvoid = 
104857600
2024-04-20 16:30:54,345 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 
26209320(104837280); length = 5077/6553600
2024-04-20 16:30:54,438 INFO mapred.MapTask: Finished spill 0
2024-04-20 16:30:54,455 INFO mapred.Task: 
Task:attempt_local1472617565_0001_m_000000_0 is done. And is in the process of committing
2024-04-20 16:30:54,460 INFO mapred.LocalJobRunner: map
2024-04-20 16:30:54,461 INFO mapred.Task: Task 
'attempt_local1472617565_0001_m_000000_0' done.
2024-04-20 16:30:54,475 INFO mapred.Task: Final Counters for 
attempt_local1472617565_0001_m_000000_0: Counters: 24
File System Counters
FILE: Number of bytes read=4158
FILE: Number of bytes written=642931
FILE: Number of read operations=0
FILE: Number of large read operations=0
FILE: Number of write operations=0
HDFS: Number of bytes read=143084
HDFS: Number of bytes written=0
HDFS: Number of read operations=5
HDFS: Number of large read operations=0
HDFS: Number of write operations=1
HDFS: Number of bytes read erasure-coded=0
Map-Reduce Framework
Map input records=1295
Map output records=1270
Map output bytes=22493
Map output materialized bytes=25
Input split bytes=101
Combine input records=1270
Combine output records=1
Spilled Records=1
Failed Shuffles=0
Merged Map outputs=0
GC time elapsed (ms)=203
Total committed heap usage (bytes)=470286336
File Input Format Counters
Bytes Read=143084
2024-04-20 16:30:54,475 INFO mapred.LocalJobRunner: Finishing task: 
attempt_local1472617565_0001_m_000000_0
2024-04-20 16:30:54,476 INFO mapred.LocalJobRunner: map task executor complete.
2024-04-20 16:30:54,479 INFO mapred.LocalJobRunner: Waiting for reduce tasks
2024-04-20 16:30:54,480 INFO mapred.LocalJobRunner: Starting task: 
attempt_local1472617565_0001_r_000000_0
2024-04-20 16:30:54,488 INFO output.FileOutputCommitter: File Output Committer Algorithm 
version is 2
2024-04-20 16:30:54,488 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup 
_temporary folders under output directory:false, ignore cleanup failures: false
2024-04-20 16:30:54,489 INFO mapred.Task: Using ResourceCalculatorProcessTree : [ ]
2024-04-20 16:30:54,493 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: 
org.apache.hadoop.mapreduce.task.reduce.Shuffle@723b2b5e
2024-04-20 16:30:54,494 WARN impl.MetricsSystemImpl: JobTracker metrics system already 
initialized!
2024-04-20 16:30:54,545 INFO reduce.MergeManagerImpl: MergerManager: 
memoryLimit=1280101504, maxSingleShuffleLimit=320025376, mergeThreshold=844867008, 
ioSortFactor=10, memToMemMergeOutputsThreshold=10
2024-04-20 16:30:54,548 INFO reduce.EventFetcher: 
attempt_local1472617565_0001_r_000000_0 Thread started: EventFetcher for fetching Map 
Completion Events
2024-04-20 16:30:54,585 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of 
map attempt_local1472617565_0001_m_000000_0 decomp: 21 len: 25 to MEMORY
2024-04-20 16:30:54,587 INFO reduce.InMemoryMapOutput: Read 21 bytes from map-output 
for attempt_local1472617565_0001_m_000000_0
2024-04-20 16:30:54,589 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output 
of size: 21, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->21
2024-04-20 16:30:54,590 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning
2024-04-20 16:30:54,591 INFO mapred.LocalJobRunner: 1 / 1 copied.
2024-04-20 16:30:54,591 INFO reduce.MergeManagerImpl: finalMerge called with 1 inmemory map-outputs and 0 on-disk map-outputs
2024-04-20 16:30:54,616 INFO mapred.Merger: Merging 1 sorted segments
2024-04-20 16:30:54,616 INFO mapred.Merger: Down to the last merge-pass, with 1 segments 
left of total size: 6 bytes
2024-04-20 16:30:54,621 INFO reduce.MergeManagerImpl: Merged 1 segments, 21 bytes to 
disk to satisfy reduce memory limit
2024-04-20 16:30:54,621 INFO reduce.MergeManagerImpl: Merging 1 files, 25 bytes from disk
2024-04-20 16:30:54,622 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from 
memory into reduce
2024-04-20 16:30:54,622 INFO mapred.Merger: Merging 1 sorted segments
2024-04-20 16:30:54,623 INFO mapred.Merger: Down to the last merge-pass, with 1 segments 
left of total size: 6 bytes
2024-04-20 16:30:54,623 INFO mapred.LocalJobRunner: 1 / 1 copied.
2024-04-20 16:30:54,628 INFO mapreduce.Job: Job job_local1472617565_0001 running in uber 
mode : false
2024-04-20 16:30:54,629 INFO mapreduce.Job: map 100% reduce 0%
2024-04-20 16:30:54,694 INFO Configuration.deprecation: mapred.skip.on is deprecated. 
Instead, use mapreduce.job.skiprecords
2024-04-20 16:30:54,800 INFO mapred.Task: 
Task:attempt_local1472617565_0001_r_000000_0 is done. And is in the process of committing
2024-04-20 16:30:54,803 INFO mapred.LocalJobRunner: 1 / 1 copied.
2024-04-20 16:30:54,803 INFO mapred.Task: Task 
attempt_local1472617565_0001_r_000000_0 is allowed to commit now
2024-04-20 16:30:54,834 INFO output.FileOutputCommitter: Saved output of task 
'attempt_local1472617565_0001_r_000000_0' to hdfs://localhost:9000/mydata/output1
2024-04-20 16:30:54,835 INFO mapred.LocalJobRunner: reduce > reduce
2024-04-20 16:30:54,835 INFO mapred.Task: Task 
'attempt_local1472617565_0001_r_000000_0' done.
2024-04-20 16:30:54,836 INFO mapred.Task: Final Counters for 
attempt_local1472617565_0001_r_000000_0: Counters: 30
File System Counters
FILE: Number of bytes read=4240
FILE: Number of bytes written=642956
FILE: Number of read operations=0
FILE: Number of large read operations=0
FILE: Number of write operations=0
HDFS: Number of bytes read=143084
HDFS: Number of bytes written=16
HDFS: Number of read operations=10
HDFS: Number of large read operations=0
HDFS: Number of write operations=3
HDFS: Number of bytes read erasure-coded=0
Map-Reduce Framework
Combine input records=0
Combine output records=0
Reduce input groups=1
Reduce shuffle bytes=25
Reduce input records=1
Reduce output records=1
Spilled Records=1
Shuffled Maps =1
Failed Shuffles=0
Merged Map outputs=1
GC time elapsed (ms)=0
Total committed heap usage (bytes)=470286336
Shuffle Errors
BAD_ID=0
CONNECTION=0
IO_ERROR=0
WRONG_LENGTH=0
WRONG_MAP=0
WRONG_REDUCE=0
File Output Format Counters
Bytes Written=16
2024-04-20 16:30:54,836 INFO mapred.LocalJobRunner: Finishing task: 
attempt_local1472617565_0001_r_000000_0
2024-04-20 16:30:54,836 INFO mapred.LocalJobRunner: reduce task executor complete.
2024-04-20 16:30:55,631 INFO mapreduce.Job: map 100% reduce 100%
2024-04-20 16:30:55,632 INFO mapreduce.Job: Job job_local1472617565_0001 completed 
successfully
2024-04-20 16:30:55,641 INFO mapreduce.Job: Counters: 36
File System Counters
FILE: Number of bytes read=8398
FILE: Number of bytes written=1285887
FILE: Number of read operations=0
FILE: Number of large read operations=0
FILE: Number of write operations=0
HDFS: Number of bytes read=286168
HDFS: Number of bytes written=16
HDFS: Number of read operations=15
HDFS: Number of large read operations=0
HDFS: Number of write operations=4
HDFS: Number of bytes read erasure-coded=0
Map-Reduce Framework
Map input records=1295
Map output records=1270
Map output bytes=22493
Map output materialized bytes=25
Input split bytes=101
Combine input records=1270
Combine output records=1
Reduce input groups=1
Reduce shuffle bytes=25
Reduce input records=1
Reduce output records=1
Spilled Records=2
Shuffled Maps =1
Failed Shuffles=0
Merged Map outputs=1
GC time elapsed (ms)=203
Total committed heap usage (bytes)=940572672
Shuffle Errors
BAD_ID=0
CONNECTION=0
IO_ERROR=0
WRONG_LENGTH=0
WRONG_MAP=0
WRONG_REDUCE=0
File Input Format Counters
Bytes Read=143084
File Output Format Counters
Bytes Written=16
hduser@student-ThinkCentre-M700:~$ hdfs dfs -cat /mydata/output1/*
2024-04-20 16:31:24,109 WARN util.NativeCodeLoader: Unable to load native-hadoop library 
for your platform... using builtin-java classes where applicable
10.82.30.199 63
hduser@student-ThinkCentre-M700:~$